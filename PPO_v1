#보상이 너무 적고 패널티가 너무 심해 학습실패
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from gymnasium.wrappers import TransformObservation

# 환경 정의
class AppleGameEnv(gym.Env):
    def __init__(self):
        super(AppleGameEnv, self).__init__()
        self.rows = 10
        self.cols = 17
        self.board = np.random.randint(1, 10, (self.rows, self.cols))
        
        # 모든 가능한 직사각형 영역 (r1, c1, r2, c2) 계산
        self.valid_actions = []
        for r1 in range(self.rows):
            for c1 in range(self.cols):
                for r2 in range(r1, self.rows):
                    for c2 in range(c1, self.cols):
                        self.valid_actions.append((r1, c1, r2, c2))
        self.num_actions = len(self.valid_actions)
        self.action_space = spaces.Discrete(self.num_actions)
        # 관측 공간: (10, 17, 1) 형태, 원래 값은 0~9 (정수)
        self.observation_space = spaces.Box(low=0, high=9, shape=(self.rows, self.cols, 1), dtype=np.int32)
        
        self.max_time = 120  # 최대 스텝 수
        self.current_time = 0
        self.score = 0
        self.chain = 0     # 연속 유효 조합 체인 카운터

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.board = np.random.randint(1, 10, (self.rows, self.cols))
        self.current_time = 0
        self.score = 0
        self.chain = 0
        # 관측은 (10, 17, 1) 형태, int32
        return self.board.copy().astype(np.int32).reshape(self.rows, self.cols, 1), {}

    def step(self, action):
        self.current_time += 1
        reward = -1
        r1, c1, r2, c2 = self.valid_actions[action]
        region = self.board[r1:r2+1, c1:c2+1]
        selected_numbers = region[region != 0]
        if selected_numbers.size > 0:
            total = int(np.sum(selected_numbers))
            if total == 10:
                num_removed = selected_numbers.size
                base_reward = num_removed + 1
                self.chain += 1
                chain_bonus = 0.2 * self.chain
                reward = base_reward + chain_bonus
                self.score += reward
                region[region != 0] = 0
                self.board[r1:r2+1, c1:c2+1] = region
            else:
                diff = abs(total - 10)
                if diff <= 2:
                    reward = 0.5 * (2 - diff)
                else:
                    reward = -1 * diff
                self.chain = 0  # 실패 시 체인 리셋
        terminated = False
        truncated = False
        if self.current_time >= self.max_time:
            terminated = True
            #시간보너스
            if self.score >= 50:
                reward += 10
        info = {"score": self.score, "time": self.current_time, "chain": self.chain}
        return self.board.copy().astype(np.int32).reshape(self.rows, self.cols, 1), reward, terminated, truncated, info

    def render(self, mode="human"):
        print("Time:", self.current_time, "Score:", self.score, "Chain:", self.chain)
        print(self.board.reshape(self.rows, self.cols))


# -------------------------------------------------------
# 환경 래퍼: 관측값을 float32로 변환하고 0~1로 정규화
new_obs_space = spaces.Box(low=0.0, high=1.0, shape=(10, 13, 1), dtype=np.float32)
env = AppleGameEnv()
env = TransformObservation(env, lambda obs: obs.astype(np.float32)/9.0, observation_space=new_obs_space)

# -------------------------------------------------------
# PPO 학습
from stable_baselines3 import PPO
# CNN 기반 정책 사용 ("CnnPolicy")
model = PPO("CnnPolicy", env, verbose=1,
            learning_rate=0.0003,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            ent_coef=0.01)

# 총 300,000 타임스텝 동안 학습
model.learn(total_timesteps=300000)
model.save("apple_game_ppo_model")
print("모델 학습 및 저장 완료.")

# -------------------------------------------------------
# 저장된 모델로 자동 플레이
obs, _ = env.reset()
env.render()
terminated = False
while not terminated:
    action, _ = model.predict(obs)
    obs, reward, terminated, truncated, info = env.step(action)
    env.render()
    print("Action:", env.unwrapped.valid_actions[action], "Reward:", reward, "Info:", info)
